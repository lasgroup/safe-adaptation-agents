defaults:
  agent: vanilla_policy_gradients
  safe: True
  robot: point
  cost_limit: 25
  log_dir: results
  seed: 0
  time_limit: 1000
  epochs: 200
  eval_every: 5
  eval_trials: 5
  train_driver: {adaptation_steps: 5000, query_steps: 2000}
  test_driver: {adaptation_steps: 5000, query_steps: 2000}
  action_repeat: 1
  render_episodes: 1
  render_options: {camera_id: 'fixedfar'}
  render_lidar_and_collision: False
  render_mode: rgb_array
  jit: True
  precision: 32
  parallel_envs: 10
  task_batch_size: 10

no_adaptation:
  task: go_to_goal
  train_driver: {adaptation_steps: 30000, query_steps: 0}
  test_driver: {adaptation_steps: 0, query_steps: 10000}
  eval_trials: 1
  task_batch_size: 1
  epochs: 334

vanilla_policy_gradients:
  entropy_regularization: 0.
  vf_iters: 5
  pi_iters: 1
  actor: {layers: [32, 32], min_stddev: 0.01, max_stddev: 10., activation: jnn.tanh, initialization: glorot, squash: False, heteroscedastic: False}
  critic: {layers: [32, 32], dist: normal, activation: jnn.tanh, name: 'critic', initialization: glorot}
  discount: 0.99
  lambda_: 0.97
  num_trajectories: 30
  actor_opt: {lr: 0.01, eps: 1e-8}
  critic_opt: {lr: 0.01, eps: 1e-8}

ppo_lagrangian:
  entropy_regularization: 0.
  vf_iters: 80
  pi_iters: 80
  actor: {layers: [256, 256], min_stddev: 0.01, max_stddev: 10., activation: jnn.tanh, initialization: glorot, squash: False, heteroscedastic: False}
  critic: {layers: [256, 256], dist: normal, activation: jnn.tanh, name: 'critic', initialization: glorot}
  discount: 0.99
  lambda_: 0.97
  cost_discount: 0.99
  num_trajectories: 30
  actor_opt: {lr: 3e-4, eps: 1e-5, clip: 0.5}
  critic_opt: {lr: 1e-3, eps: 1e-5, clip: 0.5}
  lagrangian_opt: {lr: 5e-2, eps: 1e-5, clip: 0.5}
  clip_ratio: 0.2
  kl_margin: 1.2
  target_kl: 0.01
  initial_lagrangian: 1.

cpo:
  entropy_regularization: 0.
  vf_iters: 80
  actor: {layers: [256, 256], min_stddev: 0.01, max_stddev: 10., activation: jnn.tanh, initialization: glorot, squash: False, heteroscedastic: False}
  critic: {layers: [256, 256], dist: normal, activation: jnn.tanh, name: 'critic', initialization: glorot}
  discount: 0.99
  lambda_: 0.97
  cost_discount: 0.99
  num_trajectories: 30
  actor_opt: {}
  critic_opt: {lr: 1e-3, eps: 1e-5, clip: 0.5}
  target_kl: 0.01
  backtrack_coeff: 0.8
  backtrack_iters: 10
  damping_coeff: 0.1
  margin_lr: 0.05

domain_randomization:
  train_driver: {adaptation_steps: 10000, query_steps: 10000}
  test_driver: {adaptation_steps: 10000, query_steps: 10000}
  eval_trials: 1
  task_batch_size: 10
  epochs: 1000

maml_ppo_lagrangian:
  entropy_regularization: 0.
  vf_iters: 80
  pi_iters: 80
  actor: {layers: [256, 256], min_stddev: 0.01, max_stddev: 10., activation: jnn.tanh, initialization: glorot, squash: False, heteroscedastic: False}
  critic: {layers: [256, 256], dist: normal, activation: jnn.tanh, name: 'critic', initialization: glorot}
  discount: 0.99
  lambda_: 0.97
  cost_discount: 0.99
  num_trajectories: 10
  num_query_trajectories: 10
  actor_opt: {lr: 1e-4, eps: 1e-5}
  critic_opt: {lr: 1e-3, eps: 1e-5}
  lagrangian_opt: {lr: 5e-2, eps: 1e-5}
  clip_ratio: 0.2
  kl_margin: 1.2
  target_kl: 0.01
  initial_lagrangian: 1.
  lagrangian_inner_lr: 0.1
  policy_inner_lr: 0.001
  inner_lr_opt: {lr: 0.}
  inner_steps: 1

maml_cpo:
  entropy_regularization: 0.
  vf_iters: 80
  actor: {layers: [256, 256], min_stddev: 0.01, max_stddev: 10., activation: jnn.tanh, initialization: glorot, squash: False, heteroscedastic: False}
  critic: {layers: [256, 256], dist: normal, activation: jnn.tanh, name: 'critic', initialization: glorot}
  discount: 0.99
  lambda_: 0.97
  cost_discount: 0.99
  num_trajectories: 10
  num_query_trajectories: 10
  actor_opt: {}
  critic_opt: {lr: 1e-3, eps: 1e-5, clip: 0.5}
  target_kl: 0.01
  backtrack_coeff: 0.8
  backtrack_iters: 10
  damping_coeff: 0.1
  margin_lr: 0.05
  policy_inner_lr: 0.1
  inner_steps: 1

rl2_cpo:
  entropy_regularization: 0.
  vf_iters: 80
  actor: {layers: [256, 256], min_stddev: 0.01, max_stddev: 10., activation: jnn.tanh, initialization: glorot, squash: False, heteroscedastic: False}
  critic: {layers: [256, 256], dist: normal, activation: jnn.tanh, name: 'critic', initialization: glorot}
  discount: 0.99
  lambda_: 0.97
  cost_discount: 0.99
  num_trajectories: 20
  episodes_per_task: 2
  hidden_size: 256
  actor_opt: {}
  critic_opt: {lr: 1e-3, eps: 1e-5, clip: 0.5}
  target_kl: 0.01
  backtrack_coeff: 0.8
  backtrack_iters: 10
  damping_coeff: 0.1
  margin_lr: 0.05

la_mbda:
  precision: 16
  prefill: 5000
  train_every: 1000
  update_steps: 100
  action_repeat: 1
  replay_buffer: {capacity: 1000, batch_size: 32, sequence_length: 50}
  # World Model
  rssm: {hidden: 200, deterministic_size: 200, stochastic_size: 30}
  model_opt: {lr: 1e-4, eps: 1e-5, clip: 100}
  swag: {start_averaging: 500, average_period: 200, max_num_models: 20, decay: 0.8, scale: 1., learning_rate_factor: 5.}
  encoder: {depth: 32, kernels: [4, 4, 4, 4]}
  decoder: {depth: 32, kernels: [5, 5, 6, 6]}
  reward: {layers: [400, 400]}
  cost: {layers: [400, 400]}
  free_kl: 3.0
  kl_scale: 1.0
  kl_mix: 0.8
  # Actor-Critic
  posterior_samples: 5
  actor: {layers: [400, 400, 400, 400], min_stddev: 0.01, max_stddev: 10., activation: jnn.elu, initialization: glorot, squash: True, heteroscedastic: True}
  critic: {layers: [400, 400, 400], dist: normal, activation: jnn.elu, name: 'critic', initialization: glorot}
  actor_opt: {lr: 8e-5, eps: 1e-5, clip: 5}
  critic_opt: {lr: 8e-5, eps: 1e-5, clip: 1}
  safety_critic_opt: {lr: 2e-4, eps: 1e-5, clip: 50}
  augmented_lagrangian: {initial_lagrangian: 1e-6, initial_penalty: 5e-9}
  penalty_power_factor: 1e-5
  cost_weight: 100.
  discount: 0.99
  cost_discount: 0.995
  lambda_: 0.95
  sample_horizon: 15
  evaluate_model: True


